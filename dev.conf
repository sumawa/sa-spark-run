database {
  host = "127.0.0.1"
  port = 5432
  name = "sparkrun"
  user = "sauser"
  pass = "Test123!"
  type = "postgresql" // use one of mysql, postgresql or sqlserver
  pool-size = 2
}

standaloneConf {
    rest-host: "http://127.0.0.1:6066",
    standalone-param {
      app-resource: "/opt/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar",
      spark-properties: {
        "spark.executor.memory": "1g",
        "spark.num.executors" : "1"
        "spark.master": "spark://127.0.0.1:7077",
        "spark.driver.memory": "1g",
        "spark.driver.cores": "1",
        "spark.executor.cores": "2",
        "spark.eventLog.enabled": "true",
        "spark.app.name": "Spark REST API- NEW PI",
        "spark.submit.deployMode": "cluster",
        "spark.driver.extraClassPath": "/opt/spark-2.4.0-bin-hadoop2.7/conf/:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-core_2.11-2.4.0.jar"
        "spark.jars": "/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-core_2.11-2.4.0.jar,/opt/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar",
        "spark.driver.supervise": "false"
      },
      client-spark-version: "2.4.0",
      main-class: "org.apache.spark.examples.SparkPi",
      environment-variables: {
        SPARK_ENV_LOADED: "1"
      },
      action: "CreateSubmissionRequest",
      app-args: [
        "80"
      ]
    }

}

yarnConf {
  java-path = "/usr/bin/java -Dspark.yarn.am.waitTime=600s"
  java-home = "/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home"
  spark-home = "/opt/spark-2.4.0-bin-hadoop2.7"
  hadoop-base = "/opt/hadoop-2.7.3"
  max-attempts = 1
  default-queue = "default"
  connection-timeout = 5
  classpath = "{{PWD}}<CPS>"
  spark-jars = "/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-core_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-repl_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-sql_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.0.jar:/optjars/spark-tags_2.11-2.4.0-tests.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-tags_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/metrics-json-3.1.5.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/*"
  yarn-hadoop = [
          "/etc/hadoop/"
          , "/share/hadoop/common/*"
          , "/share/hadoop/common/lib/*"
          , "/share/hadoop/hdfs/*"
          , "/share/hadoop/hdfs/lib/*"
          , "/share/hadoop/yarn/*"
          , "/share/hadoop/yarn/lib/*"
          , "/share/hadoop/mapreduce/*"
        ]
  yarn-spark = [
      "/jars/netty-3.9.9.Final.jar"
      , "/jars/netty-all-4.1.17.Final.jar"
      , "/jars/antlr4-runtime-4.7.jar"
      , "/jars/*"
  ]
}

daemon {
  cores: 1
  memory: 1500
  executors: 1
  jar-path: "hdfs:///sparkrun/books/spark-examples_2.11-2.4.0.jar"
  spark-params = "spark.yarn.appMasterEnv.SPARK_DIST_CLASSPATH=/opt/spark-2.4.0-bin-hadoop2.7/jars/*,spark.yarn.executorEnv.SPARK_DIST_CLASSPATH=spark.driver.extraClassPath=/opt/spark-2.4.0-bin-hadoop2.7/netty-3.9.9.Final.jar:/opt/spark-2.4.0-bin-hadoop2.7/netty-all-4.1.17.Final.jar,/opt/spark-2.4.0-bin-hadoop2.7/jars/*,spark.yarn.dist.jars=hdfs:///sparkrun/books/spark-examples_2.11-2.4.0.jar,spark.yarn.archive=hdfs:///sparkrun/books/yarn-archive.zip,spark.executor.extraClassPath=/opt/spark-2.4.0-bin-hadoop2.7/jars/*,spark.driver.extraClassPath=/opt/spark-2.4.0-bin-hadoop2.7/netty-3.9.9.Final.jar:/opt/spark-2.4.0-bin-hadoop2.7/netty-all-4.1.17.Final.jar:/opt/spark-2.4.0-bin-hadoop2.7/jars/*,spark.app.name=RS_RESEARCH,master=yarn,spark.submit.deployMode=cluster,spark.driver.memory=3g,spark.driver.cores=1,spark.executor.memory=3g,spark.executor.cores=1,spark.executor.instances=1,spark.sql.crossJoin.enabled=true"
  http-params = "http,3.228.128.195,8081,/develop,v1"
  base-dir = "/sparkrun/books/"
  jdbc-params = "3.228.128.195,3306,rsdev,rsdev,test123,break_metric"
  assembly-path = "hdfs:///opt/test/spark-examples_2.11-2.4.0.jar"
  periodicity: 5
}

yarn {
  client-config = {
    "fs.defaultFS" : "hdfs://127.0.0.1:50070"
    "yarn.resourcemanager.hostname" = "127.0.0.1"
    "yarn.resourcemanager.webapp.address"	= "127.0.0.1:10800"

  }
}

old_yarn_MAPR {
    client-config = {
        "fs.defaultFS" : "maprfs://DEV.SOMECOMPANY.COM"
        "yarn.resourcemanager.ha.custom-ha-enabled": "true"
        "yarn.client.failover-proxy-provider": "org.apache.hadoop.yarn.client.MapRZKBasedRMFailoverProxyProvider"
        "yarn.resourcemanager.recovery.enabled": "true"
        "yarn.resourcemanager.ha.custom-ha-rmaddressfinder": "org.apache.hadoop.yarn.client.MapRZKBasedRMAddressFinder"
    }
}
